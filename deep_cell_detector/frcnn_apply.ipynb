{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"#-----specify device gpu or cpu\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from optparse import OptionParser\n",
    "import time\n",
    "from keras_frcnn import config\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import roi_helpers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from keras_frcnn.wwk_parser import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_path='/home/zoro/Desktop/nmumg_data'\n",
    "main_path=dir_path+'/train_test_data'\n",
    "data_path=dir_path+'/train_test_data/test'\n",
    "img_path=dir_path+'/frcnn_predict' #-----for prediction-------\n",
    "path_output=dir_path+'/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg\n"
     ]
    }
   ],
   "source": [
    "C=config.Config()\n",
    "if C.network == 'resnet50':\n",
    "\timport keras_frcnn.resnet as nn\n",
    "elif C.network == 'vgg':\n",
    "\timport keras_frcnn.vgg as nn\n",
    "\n",
    "print C.network\n",
    "# turn off any data augmentation at test timeuse_horizontal_flips\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------note wwk---------- ratio is C.im_size/train_img_size\n",
    "def format_img_size(img, C):\n",
    "\t\"\"\" formats the image size based on config \"\"\"\n",
    "\t(height,width,_) = img.shape\n",
    "\t    \n",
    "\tratio=float(C.im_size)/float(C.train_img_size)\n",
    "\tnew_width = int(ratio * width)\n",
    "\tnew_height = int(ratio*height)\n",
    "\tprint img.shape\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\timg = np.reshape(img,(new_width,new_height,1))    \n",
    "\treturn img, ratio\t\n",
    "\n",
    "def format_img_channels(img, C):\n",
    "\t\"\"\" formats the image channels based on config \"\"\"\n",
    "# \timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "# \timg[:, :, 0] -= C.img_channel_mean[0]\n",
    "# \timg[:, :, 1] -= C.img_channel_mean[1]\n",
    "# \timg[:, :, 2] -= C.img_channel_mean[2]\n",
    "# \timg /= C.img_scaling_factor\n",
    "\timg[:, :, 0] -= C.img_mean\n",
    "#\timg = np.transpose(img, (2, 0, 1))\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img\n",
    "\n",
    "def format_img(img, C):\n",
    "\t\"\"\" formats an image for model prediction based on config \"\"\"\n",
    "\timg, ratio = format_img_size(img, C)\n",
    "\timg = format_img_channels(img, C)\n",
    "\treturn img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "\treal_x1 = int(round(x1 // ratio))\n",
    "\treal_y1 = int(round(y1 // ratio))\n",
    "\treal_x2 = int(round(x2 // ratio))\n",
    "\treal_y2 = int(round(y2 // ratio))\n",
    "\n",
    "\treturn (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "{'cell': 0, 'bg': 1}\n"
     ]
    }
   ],
   "source": [
    "all_imgs, classes_count, class_mapping = get_data(main_path,data_path)\n",
    "\n",
    "if 'bg' not in classes_count:\n",
    "\tclasses_count['bg'] = 0\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "C.class_mapping = class_mapping\n",
    "print class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'cell', 1: 'bg'}\n"
     ]
    }
   ],
   "source": [
    "if 'bg' not in class_mapping:#background\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\n",
    "#C.num_rois = int(options.num_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 1)\n"
     ]
    }
   ],
   "source": [
    "if C.network == 'resnet50':\n",
    "\tnum_features = 1024\n",
    "elif C.network == 'vgg':\n",
    "\tnum_features = 512\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "\tinput_shape_img = (1, None, None)\n",
    "\tinput_shape_features = (num_features, None, None)\n",
    "else:\n",
    "\tinput_shape_img = (None, None, 1)\n",
    "\tinput_shape_features = (None, None, num_features)\n",
    "\n",
    "print input_shape_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from model_frcnn_nmumg_ellipse.vgg.hdf5\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_out_exact0.tif\n",
      "(384, 384, 1)\n"
     ]
    }
   ],
   "source": [
    "all_imgs = []\n",
    "\n",
    "classes = {}\n",
    "all_box=[]\n",
    "\n",
    "bbox_threshold = 0.5#---note wwk--- probability threshold for classification\n",
    "\n",
    "overlap_thresh=0.2# for non_max_suppression_fast\n",
    "\n",
    "visualise = True\n",
    "\n",
    "for idx, img_name in enumerate(sorted(os.listdir(img_path))):\n",
    "\tif not img_name.lower().endswith(('.tif', '.tiff')):\n",
    "\t\tcontinue\n",
    "\tprint(img_name)\n",
    "\tst = time.time()\n",
    "\tfilepath = os.path.join(img_path,img_name)\n",
    "\n",
    "\timg = cv2.imread(filepath,cv2.COLOR_BGR2GRAY)\n",
    "\t#img = bg_correction(img,ordr=1)#-----------backgroud correction--------------\n",
    "\timg=np.reshape(img,(img.shape[0],img.shape[1],1))\n",
    "    \n",
    "\t#----note wwk----we train on real size image. cell in large images and the train images have the same size\n",
    "\t#----so when make prediction on images whose size is quite different from the training size, should not resize\n",
    "\tX, ratio = format_img(img, C)\n",
    "# \tif K.image_dim_ordering() == 'tf':\n",
    "# \t\tX = np.transpose(X, (0, 2, 3, 1))\n",
    "# \tX=np.expand_dims(img, axis=0)#this is the alternative of X,ratio=format_img(img,C)\n",
    "# \tratio=1 \n",
    "    \n",
    "\t# get the feature maps and output from the RPN\n",
    "\t[Y1, Y2, F] = model_rpn.predict(X)\n",
    "\t\n",
    "\t#----note wwk--- R shape(:,4)  x1,y1,x2,y2 of each anchor box   \n",
    "\tR = roi_helpers.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "\n",
    "\t# convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "\tR[:, 2] -= R[:, 0]\n",
    "\tR[:, 3] -= R[:, 1]\n",
    "\n",
    "\t# apply the spatial pyramid pooling to the proposed regions\n",
    "\tbboxes = {}\n",
    "\tprobs = {}\n",
    "\n",
    "\tfor jk in range(R.shape[0]//C.num_rois + 1): # ---note wwk---calculate C.num_rois at the same time\n",
    "\t\tROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "\t\t#print ROIs.shape        \n",
    "\t\tif ROIs.shape[1] == 0:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tif jk == R.shape[0]//C.num_rois:#if jk==R.shape[0]//C.num_rois, ROIs.shape is not (1,x,4),x<C.num_rois\n",
    "\t\t\t#pad R\n",
    "\t\t\tcurr_shape = ROIs.shape\n",
    "\t\t\t#print curr_shape            \n",
    "\t\t\ttarget_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "\t\t\tROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "\t\t\tROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "\t\t\tROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "\t\t\tROIs = ROIs_padded\n",
    "\n",
    "\t\t[P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "\t\t#print P_cls.shape\n",
    "\t\tfor ii in range(P_cls.shape[1]):\n",
    "\t\t\t#----note wwk--- if <threshold or the largest probability is background       \n",
    "\t\t\tif np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tcls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "\t\t\tif cls_name not in bboxes:\n",
    "\t\t\t\tbboxes[cls_name] = []\n",
    "\t\t\t\tprobs[cls_name] = []\n",
    "\n",
    "\t\t\t(x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "\t\t\tcls_num = np.argmax(P_cls[0, ii, :])\n",
    "\t\t\ttry:\n",
    "\t\t\t\t(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "\t\t\t\ttx /= C.classifier_regr_std[0]\n",
    "\t\t\t\tty /= C.classifier_regr_std[1]\n",
    "\t\t\t\ttw /= C.classifier_regr_std[2]\n",
    "\t\t\t\tth /= C.classifier_regr_std[3]\n",
    "\t\t\t\tx, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t\tbboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "\t\t\tprobs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\tprint type(bboxes)\n",
    "\tall_dets = []\n",
    "\n",
    "\tfor key in bboxes:\n",
    "\t\tprint(key)        \n",
    "\t\tbbox = np.array(bboxes[key])        \n",
    "\t\timg=np.reshape(img,(img.shape[0],img.shape[1]))\n",
    "\t\tfig, ax = plt.subplots(figsize=(10, 6))\n",
    "\t\tax.imshow(img)\n",
    "\t\tnew_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=overlap_thresh)\n",
    "\t\treal_boxes=np.zeros(new_boxes.shape)\n",
    "\t\tfor jk in range(new_boxes.shape[0]):\n",
    "\t\t\t(x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "\t\t\t(real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\t\t\tprint real_x1, real_y1, real_x2, real_y2\n",
    "\t\t\treal_boxes[jk,:]=real_x1,real_y1,real_x2,real_y2\n",
    "# \t\t\treal_boxes[jk,1]=real_y1 \n",
    "# \t\t\treal_boxes[jk,2]=real_x2\n",
    "# \t\t\treal_boxes[jk,3]=real_y2\n",
    "#\t\t\tcv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n",
    "\n",
    "\n",
    "\t\t\trect = mpatches.Rectangle((real_x1,real_y1),real_x2-real_x1, real_y2-real_y1,fill = False, edgecolor = 'red', linewidth = 2)\n",
    "\t\t\tax.add_patch(rect)\n",
    "\t\t\tplt.axis('off')        \n",
    "\t\t\tplt.savefig(path_output+'/box_predict.tif',bbox_inches='tight',format='tif',dpi=300)\n",
    "            \n",
    "\t\t\ttextLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "\t\t\tall_dets.append((key,100*new_probs[jk]))\n",
    "\t\tplt.show()       \n",
    "\t\treal_boxes=real_boxes.astype(int)\n",
    "\t\tall_box.append(real_boxes)\n",
    "\tprint('Elapsed time = {}'.format(time.time() - st))\n",
    "\tprint(all_dets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[368, 520, 424, 568],\n",
      "       [520, 216, 560, 256],\n",
      "       [360, 448, 416, 504],\n",
      "       [448, 464, 504, 504],\n",
      "       [536, 384, 576, 432],\n",
      "       [496, 504, 568, 552],\n",
      "       [352, 376, 400, 416],\n",
      "       [520, 280, 576, 344],\n",
      "       [416, 344, 480, 392],\n",
      "       [240, 320, 296, 384],\n",
      "       [544,  56, 584, 104],\n",
      "       [168, 576, 208, 632],\n",
      "       [296, 432, 336, 472],\n",
      "       [336, 592, 392, 640],\n",
      "       [472, 312, 528, 384],\n",
      "       [576, 264, 608, 312],\n",
      "       [440, 144, 504, 192],\n",
      "       [488,  72, 528, 128],\n",
      "       [416, 224, 496, 312],\n",
      "       [232, 560, 272, 608],\n",
      "       [272, 520, 320, 576],\n",
      "       [336, 320, 384, 360],\n",
      "       [528, 128, 616, 192],\n",
      "       [224, 384, 272, 448],\n",
      "       [376, 168, 432, 216],\n",
      "       [312, 488, 352, 536],\n",
      "       [192, 424, 232, 488],\n",
      "       [592, 320, 640, 360],\n",
      "       [224, 464, 280, 528],\n",
      "       [432, 536, 512, 592],\n",
      "       [400, 408, 480, 448],\n",
      "       [168, 512, 208, 560],\n",
      "       [304, 208, 392, 256],\n",
      "       [520, 448, 584, 496],\n",
      "       [592, 552, 640, 616],\n",
      "       [304, 384, 328, 408],\n",
      "       [592, 184, 640, 240],\n",
      "       [600, 408, 640, 440],\n",
      "       [200, 344, 240, 400],\n",
      "       [248, 256, 296, 296],\n",
      "       [280, 264, 360, 304],\n",
      "       [272, 600, 320, 640],\n",
      "       [584, 456, 640, 496]])]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(path_output+\"/bndbox.txt\", \"wb\") as fp: #pickling\n",
    "    pickle.dump(all_box, fp)    \n",
    "with open(path_output+\"/bndbox.txt\", \"rb\") as fp: # Unpickling\n",
    "    b = pickle.load(fp)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
